{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q llama-cpp-python \\\n",
    "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "%pip install -q llama_index llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Deploy backend\n",
    "%%capture\n",
    "!git clone https://github.com/sabeeralikp/llama_flow.git\n",
    "%cd llama_flow\n",
    "!git switch \"v0.0.2-alpha-remote-colab\"\n",
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from app.main import app\n",
    "import multiprocessing\n",
    "\n",
    "auth_token = \"paste_your_ngrok_auth_token_here\"\n",
    "\n",
    "ngrok.set_auth_token(auth_token)\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
