<div align="center">
  <img src="images/logo.jpg" alt="llama_flow Logo" width="200" height="200">
</div>

# ü¶ô‚ú® llama_flow

<p align="center">
  <a href="https://github.com/sabeeralikp/llama_flow/stargazers">
    <img alt="GitHub stars" src="https://img.shields.io/github/stars/sabeeralikp/llama_flow?style=social">
  </a>
  <a href="https://github.com/sabeeralikp/llama_flow/network/members">
    <img alt="GitHub forks" src="https://img.shields.io/github/forks/sabeeralikp/llama_flow?style=social">
  </a>
  <a href="https://github.com/sabeeralikp/llama_flow/issues">
    <img alt="GitHub issues" src="https://img.shields.io/github/issues/sabeeralikp/llama_flow">
  </a>
  <a href="https://github.com/sabeeralikp/llama_flow/pulls">
    <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/sabeeralikp/llama_flow">
  </a>
  <a href="https://github.com/sabeeralikp/llama_flow/releases">
    <img alt="GitHub release downloads" src="https://img.shields.io/github/downloads/sabeeralikp/llama_flow/total">
  </a>
</p>

**llama_flow** is an open-source application designed to develop and customize Retrieval-Augmented Generation (RAG) workflows **without code**. Easily run it locally using a variety of open-source and closed-source large language models, vector databases, embedding models, and chunking strategies.

## üöÄ Features

### üñ•Ô∏è Frontend
- Built with **[Flutter](https://flutter.dev/)**
- Available for **[Linux](https://github.com/sabeeralikp/llama_flow/releases/download/Desktop-1.0.1/llama_flow-1.0.1+2-linux.deb)** and **[Windows](https://github.com/sabeeralikp/llama_flow/releases/download/Desktop-1.0.1/llama_flow-1.0.1+2-windows-setup.exe)**
- Source Code: [llama_flow_desktop](https://github.com/sabeeralikp/llama_flow_desktop)

### üõ†Ô∏è Backend
- Dockerized for local hosting
- Utilizes **[FastAPI](https://fastapi.tiangolo.com/)** and **[llama_index](https://github.com/run-llama/llama_index)**

### Supported Components

#### Basic RAG Workflow
| Workflow                      | Status                    |
|-------------------------------|---------------------------|
| Default with [Huggingface](https://huggingface.co/)      | ‚úÖ                        |
| Support for [llamacpp](https://github.com/ggerganov/llama.cpp) and [ollama](https://ollama.com/) | ‚úÖ                        |

#### Vector DB
| Vector DB                     | Status                    |
|-------------------------------|---------------------------|
| [chromadb](https://github.com/chroma-core/chroma)                      | ‚úÖ                        |
| [waviate](https://github.com/semi-technologies/weaviate)                      | ‚è≥                        |
| [faiss](https://github.com/facebookresearch/faiss)                         | ‚è≥                        |
| [qdrant](https://github.com/qdrant/qdrant)                        | ‚è≥                        |

#### Embed Model Provider
| Embed Model Provider          | Status                    |
|-------------------------------|---------------------------|
| [Huggingface](https://huggingface.co/)                   | ‚úÖ                        |
| [Ollama](https://ollama.com/)                        | ‚è≥                        |
| [OpenAI](https://www.openai.com/)                        | ‚è≥                        |
| [Cohere](https://cohere.ai/)                        | ‚è≥                        |

#### Embed Models (Huggingface)
| Embed Model (Huggingface)      | Status                    |
|-------------------------------|---------------------------|
| [Snowflake/snowflake-arctic-embed-l](https://huggingface.co/Snowflake/snowflake-arctic-embed-l)  | ‚úÖ                 |
| [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5)        | ‚úÖ                 |
| [Snowflake/snowflake-arctic-embed-m](https://huggingface.co/Snowflake/snowflake-arctic-embed-m)  | ‚úÖ                 |
| [Snowflake/snowflake-arctic-embed-m-long](https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long) | ‚úÖ             |
| [WhereIsAI/UAE-Large-V1](https://huggingface.co/WhereIsAI/UAE-Large-V1)               | ‚úÖ                 |
| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)               | ‚úÖ                 |
| [mixedbread-ai/mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)  | ‚úÖ                 |
| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)               | ‚úÖ                 |

#### LLM Providers
| LLM Provider                  | Status                    |
|-------------------------------|---------------------------|
| [Huggingface](https://huggingface.co/models)                   | ‚úÖ                        |
| [llamacpp](https://github.com/ggerganov/llama.cpp)                      | ‚úÖ                        |
| [ollama](https://ollama.com/)                        | ‚úÖ                        |
| Huggingface API               | ‚è≥                        |
| [OpenAI](https://www.openai.com/)                        | ‚è≥                        |
| [Cohere](https://cohere.ai/)                        | ‚è≥                        |

#### Huggingface LLMs
| Huggingface LLM               | Status                    |
|-------------------------------|---------------------------|
| [microsoft/Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) | ‚úÖ                    |
| [upstage/SOLAR-10.7B-Instruct-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0) | ‚úÖ                    |
| [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)        | ‚úÖ                     |
| [Nexusflow/Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)    | ‚úÖ                     |
| [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) | ‚úÖ                  |
| [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) | ‚úÖ                   |
| [meta-llama/CodeLlama-7b-hf](https://huggingface.co/meta-llama/CodeLlama-7b-hf)       | ‚úÖ                     |
| [google/gemma-1.1-7b-it](https://huggingface.co/google/gemma-1.1-7b-it)           | ‚úÖ                     |
| [google/gemma-1.1-2b-it](https://huggingface.co/google/gemma-1.1-2b-it)           | ‚úÖ                     |

#### llamacpp LLMs
| llamacpp LLM                  | Status                    |
|-------------------------------|---------------------------|
| llama2-7b                     | ‚úÖ                        |
| llama2-13b                    | ‚úÖ                        |
| llama3-8b                     | ‚úÖ                        |

#### ollama LLMs
| ollama LLM                    | Status                    |
|-------------------------------|---------------------------|
| llama3                        | ‚úÖ                        |
| phi3                          | ‚úÖ                        |
| mistral                       | ‚úÖ                        |
| neural-chat                   | ‚úÖ                        |
| starling-lm                   | ‚úÖ                        |
| codellama                     | ‚úÖ                        |
| gemma:2b                      | ‚úÖ                        |
| gemma:7b                      | ‚úÖ                        |
| solar                         | ‚úÖ                        |

#### Chunking Strategy
| Chunking Strategy             | Status                    |
|-------------------------------|---------------------------|
| semantic-splitting            | ‚úÖ                        |
| simple-node-parser            | ‚è≥                        |
| sentence-splitting            | ‚è≥                        |
| sentence-window               | ‚è≥                        |
| token-splitting               | ‚è≥                        |
| hierarchical-splitting        | ‚è≥                        |

### Planned Features
- Advanced RAG workflow (See illustration below)
- Custom workflow with drag-and-drop functionality

<div align="center">
  <img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*Gr_JqzdpHu7enWG9.png" alt="Advanced RAG Workflow" width="500">
</div>

## üõ†Ô∏è Installation

### 1. Frontend UI Application
Download the desktop application for your OS:
- [Linux](https://github.com/sabeeralikp/llama_flow/releases/download/linux-desktop-1.0.1/llama_flow-1.0.1+2-linux.deb)
- [Windows](https://github.com/sabeeralikp/llama_flow/releases/download/linux-desktop-1.0.1/llama_flow-1.0.1+2-windows-setup.exe)

### 2. Backend Configuration

#### Option 1: Dockerized Backend
1. **Clone the repo**
   ```sh
   git clone https://github.com/sabeeralikp/llama_flow.git
   cd llama_flow
   ```
2. **Build and Run Docker Container**
   ```sh
   docker build -t llama_flow_image . 
   docker run -d --name llama_flow_container -p 8000:8000 llama_flow_image
   ```

#### Option 2: Manual Setup with FastAPI
1. **Clone the repo**
   ```sh
   git clone https://github.com/sabeeralikp/llama_flow.git
   cd llama_flow
   ```
2. **Create a virtual environment**
   ```sh
   python -m venv env
   source env/bin/activate   # On Windows use `env\Scripts\activate`
   ```
3. **Install necessary packages**
   ```sh
   pip install -r requirements.txt
   ```
4. **Run the backend with Uvicorn**
   ```sh
   uvicorn main:app --workers 4
   ```

#### Option 3: Colab/Kaggle Notebooks
- **llama_flow with Huggingface**: [Colab Notebook](https://github.com/sabeeralikp/llama_flow/blob/main/notebooks/llama_flow_huggingface_backend_colab.ipynb)
- **llama_flow with llamacpp**: [Colab Notebook](https://github.com/sabeeralikp/llama_flow/blob/main/notebooks/llama_flow_llamacpp_backend_colab.ipynb)
- **llama_flow with Ollama**: [Colab Notebook](https://github.com/sabeeralikp/llama_flow/blob/main/notebooks/llama_flow_ollama_backend_colab.ipynb)

**Instructions:**
1. Create a [Ngrok](https://ngrok.com/) account.
2. Go to the Ngrok dashboard, create an auth tunnel, copy the auth token, and paste it into Colab.
3. Open the notebook file in Colab and run all cells.
4. Copy the external link from the cell output.
5. Open the desktop application, go to settings, change the backend to remote, and paste the copied external domain link in the base URL text field.

### Additional Backend Configurations

#### Using llamacpp
```sh
pip install -q llama-cpp-python \
  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122  # For CUDA version 12.2
pip install -q llama_index llama-index-llms-llama-cpp
```

#### Using Ollama
Follow the [official Ollama installation guide](https://github.com/ollama/ollama/?tab=readme-ov-file#ollama) for different devices.
```sh
apt install lshw
curl -fsSL https://ollama.com/install.sh | sh
pip install -q llama-index-llms-ollama
```

## ü§ù Contribution Guidelines
We welcome all contributions to improve llama_flow. Please follow these steps:
1. **Fork the repository**
2. **Create a new branch**
   ```sh
   git checkout -b feature-branch
   ```
3. **Commit your changes**
   ```sh
   git commit -m 'Add some feature'
   ```
4. **Push to the branch**
   ```sh
   git push origin feature-branch
   ```
5. **Create a new Pull Request**

## üìú License
This project is licensed under the MIT License. See the [LICENSE](https://github.com/sabeeralikp/llama_flow/blob/main/LICENSE) file for details.

---

> **Stay tuned for exciting updates!** üöÄ‚ú®
